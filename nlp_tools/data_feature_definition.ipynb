{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on/inspired by a tutorial from Packt:\n",
    "# https://hub.packtpub.com/use-tensorflow-and-nlp-to-detect-duplicate-quora-questions-tutorial/\n",
    "# The data in \"quora_duplicate_questions.tsv\" is released for non-commercial use only\n",
    "# More info can be found on: https://www.quora.com/about/tos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ASSUMPTIONS**\n",
    "1. 2 questions that mean the same often share a lot of words, while 2 different questions rarely share a lot of words\n",
    "2. 2 questions that mean the same often have a small edit distance, while 2 different questions rarely have a small edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and remove unnecessary columns\n",
    "data = pd.read_csv(\"quora_duplicate_questions.tsv\", sep=\"\\t\") \\\n",
    "         .drop([\"id\", \"qid1\", \"qid2\"], axis=1)\n",
    "# Split the data set into a training (data) and testing (test) data set\n",
    "data, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LENGTH BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each sentence\n",
    "data[\"len_q1\"] = data.question1.apply(lambda x: len(str(x)))\n",
    "data[\"len_q2\"] = data.question2.apply(lambda x: len(str(x)))\n",
    "# Calculate the difference between the lengths of each pair of questions\n",
    "data[\"dif_len\"] = data.len_q1 - data.len_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the character length of each sentence (excluding spaces)\n",
    "# EXTENSION/UPGRADE also remove punctuation marks\n",
    "data[\"len_char_q1\"] = data.question1.apply(lambda x: len(str(x).replace(\" \", \"\")))\n",
    "data[\"len_char_q2\"] = data.question2.apply(lambda x: len(str(x).replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the word count of each sentence\n",
    "data[\"len_word_q1\"] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "data[\"len_word_q2\"] = data.question2.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of common words in each pair of questions\n",
    "# EXTENSION/UPGRADE try to use synonym libraries to improve this measure\n",
    "#\n",
    "data[\"common_words\"] = \\\n",
    "    data.apply(lambda x: len(set(str(x.question1).lower().split()).intersection(\n",
    "                             set(str(x.question2).lower().split()))),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length-based feature set for future reference\n",
    "fs_1 = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1',\n",
    "        'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
    "        'common_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISTANCE BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Q and W ratio of each pair of questions\n",
    "data[\"fuzz_QRatio\"] = \\\n",
    "    data.apply(lambda x: fuzz.QRatio(str(x.question1),\n",
    "                                     str(x.question2)),\n",
    "               axis=1)\n",
    "data[\"fuzz_WRatio\"] = \\\n",
    "    data.apply(lambda x: fuzz.WRatio(str(x.question1),\n",
    "                                     str(x.question2)),\n",
    "               axis=1)\n",
    "# Calculate the partial ratio of each pair of questions\n",
    "data[\"fuzz_partial_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_ratio(str(x.question1),\n",
    "                                            str(x.question2)),\n",
    "               axis=1)\n",
    "\n",
    "# Calculate the partial token set ratio of each pair of questions\n",
    "data[\"fuzz_partial_token_set_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_token_set_ratio(str(x.question1),\n",
    "                                                      str(x.question2)),\n",
    "               axis=1)\n",
    "# Calculate the partial token sort ratio of each pair of questions\n",
    "data[\"fuzz_partial_token_sort_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x.question1),\n",
    "                                                       str(x.question2)),\n",
    "               axis=1)\n",
    "\n",
    "# Calculate the token set ratio of each pair of questions\n",
    "data[\"fuzz_token_set_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.token_set_ratio(str(x.question1),\n",
    "                                              str(x.question2)),\n",
    "               axis=1)\n",
    "# Calculate the token sort ratio of each pair of questions\n",
    "data[\"fuzz_token_sort_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.token_sort_ratio(str(x.question1),\n",
    "                                               str(x.question2)),\n",
    "               axis=1)\n",
    "\n",
    "# The distance-based feature set for future reference\n",
    "fs_2 = ['fuzz_QRatio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "        'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "        'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF & SVD BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create term frequency-inverse document frequency vectorizers\n",
    "tfv_q1 = TfidfVectorizer(min_df=3,\n",
    "                         max_features=None,\n",
    "                         strip_accents='unicode',\n",
    "                         analyzer='word',\n",
    "                         token_pattern=r\"w{1,}\",\n",
    "                         ngram_range=(1, 2),\n",
    "                         use_idf=1,\n",
    "                         smooth_idf=1,\n",
    "                         sublinear_tf=1,\n",
    "                         stop_words=\"english\")\n",
    "tfv_q2 = deepcopy(tfv_q1)\n",
    "\n",
    "# Calculate the tf-idf matrices for both questions\n",
    "q1_tfidf = tfv_q1.fit_transform(data.question1.fillna(\"\"))\n",
    "q2_tfidf = tfv_q2.fit_transform(data.question2.fillna(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
