{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on/inspired by a tutorial from Packt: https://hub.packtpub.com/use-tensorflow-and-nlp-to-detect-duplicate-quora-questions-tutorial/, written by Sunith Shetty.\n",
    "The license that came with the original code can be found in the EXTERNAL_LICENSES folder.\n",
    "\n",
    "The data in \"quora_duplicate_questions.tsv\" is released for non-commercial use only by Quora.\n",
    "You can download this data from: http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv.\n",
    "More info can be found on: https://www.quora.com/about/tos.\n",
    "\n",
    "The pretrained Word2vec model from the Google News Corpus can be downloaded from: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz.\n",
    "**Warning:** the decompressed file is appr. 3.5GB in size and will be loaded into memory so 16GB of RAM and a 64-bit Python interpreter are advised.\n",
    "\n",
    "It is **important to note** that the data collected by Quora is a snapshot of a certain moment in history so questions about important events during that time may introduce biasses in the data. One way to obtain better models would be by collecting new domain-specific or general data, depending on the purpose of the recognizer.\n",
    "\n",
    "It is also **important to note** that as long as the model is based of the quora data set, the model can only be used for **non-commercial** purposes. So creating a new, independent data set is of paramount importance, should you wish to use the subsequent models for commercial purposes.\n",
    "\n",
    "quora_duplicate_questions.tsv and GoogleNews-vectors-negative300.bin.gz must be downloaded to /nlp_tools.\n",
    "\n",
    "**This code expands the quora data set with useful features to train a machine learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk import word_tokenize\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, \\\n",
    "                                   euclidean, minkowski, braycurtis\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yvesd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yvesd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary nltk packages; only run this cell if you do not have\n",
    "# these packages installed\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ASSUMPTIONS**\n",
    "1. 2 questions that mean the same often share a lot of words, while 2 different questions rarely share a lot of words.\n",
    "2. 2 questions that mean the same often have a small edit distance, while 2 different questions rarely have a small edit distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse & Expand the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and remove unnecessary columns\n",
    "data = pd.read_csv(\"quora_duplicate_questions.tsv\", sep=\"\\t\") \\\n",
    "         .drop([\"id\", \"qid1\", \"qid2\"], axis=1)\n",
    "data.question1 = data.question1.apply(lambda x: str(x))\n",
    "data.question2 = data.question2.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LENGTH BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each sentence\n",
    "data[\"len_q1\"] = data.question1.apply(lambda x: len(x))\n",
    "data[\"len_q2\"] = data.question2.apply(lambda x: len(x))\n",
    "# Calculate the difference between the lengths of each pair of questions\n",
    "data[\"diff_len\"] = data.len_q1 - data.len_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the character length of each sentence (excluding spaces)\n",
    "data[\"len_char_q1\"] = data.question1.apply(lambda x: len(x.replace(\" \", \"\")))\n",
    "data[\"len_char_q2\"] = data.question2.apply(lambda x: len(x.replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the word count of each sentence\n",
    "data[\"len_word_q1\"] = data.question1.apply(lambda x: len(x.split()))\n",
    "data[\"len_word_q2\"] = data.question2.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of common words in each pair of questions\n",
    "data[\"common_words\"] = \\\n",
    "    data.apply(lambda x: len(set(x.question1.lower().split()).intersection(\n",
    "                             set(x.question2.lower().split()))),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length-based feature set for future reference\n",
    "fs_1 = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1',\n",
    "        'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
    "        'common_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISTANCE BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Q and W ratio of each pair of questions\n",
    "data[\"fuzz_QRatio\"] = \\\n",
    "    data.apply(lambda x: fuzz.QRatio(x.question1,\n",
    "                                     x.question2),\n",
    "               axis=1)\n",
    "data[\"fuzz_WRatio\"] = \\\n",
    "    data.apply(lambda x: fuzz.WRatio(x.question1,\n",
    "                                     x.question2),\n",
    "               axis=1)\n",
    "# Calculate the partial ratio of each pair of questions\n",
    "data[\"fuzz_partial_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_ratio(x.question1,\n",
    "                                            x.question2),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the partial token set ratio of each pair of questions\n",
    "data[\"fuzz_partial_token_set_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_token_set_ratio(x.question1,\n",
    "                                                      x.question2),\n",
    "               axis=1)\n",
    "# Calculate the partial token sort ratio of each pair of questions\n",
    "data[\"fuzz_partial_token_sort_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_token_sort_ratio(x.question1,\n",
    "                                                       x.question2),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the token set ratio of each pair of questions\n",
    "data[\"fuzz_token_set_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.token_set_ratio(x.question1,\n",
    "                                              x.question2),\n",
    "               axis=1)\n",
    "# Calculate the token sort ratio of each pair of questions\n",
    "data[\"fuzz_token_sort_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.token_sort_ratio(x.question1,\n",
    "                                               x.question2),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distance-based feature set for future reference\n",
    "fs_2 = ['fuzz_QRatio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "        'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "        'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF & LSA BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create term frequency-inverse document frequency vectorizers\n",
    "tfv = TfidfVectorizer(min_df=3,\n",
    "                      max_features=None,\n",
    "                      strip_accents='unicode',\n",
    "                      analyzer='word',\n",
    "                      token_pattern=r\"\\w{1,}\",\n",
    "                      ngram_range=(1, 2),\n",
    "                      use_idf=1,\n",
    "                      smooth_idf=1,\n",
    "                      sublinear_tf=1,\n",
    "                      stop_words=\"english\")\n",
    "tfv_q1 = deepcopy(tfv)\n",
    "tfv_q2 = deepcopy(tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tf-idf matrices for both questions\n",
    "q1_tfidf = tfv_q1.fit_transform(data.question1.fillna(\"\"))\n",
    "q2_tfidf = tfv_q2.fit_transform(data.question2.fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create truncated SVD decompostions = fast but aproximate, with 180 components\n",
    "svd = TruncatedSVD(n_components=180)\n",
    "svd_q1 = TruncatedSVD(n_components=180)\n",
    "svd_q2 = TruncatedSVD(n_components=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the SVD features based on the tf-idf matrices\n",
    "question1_vectors = svd_q1.fit_transform(q1_tfidf)\n",
    "question2_vectors = svd_q2.fit_transform(q2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3rd feature set is obtained by combining the tf-idf and SVD features\n",
    "# Stack the tf-idf matrices together\n",
    "fs3_1 = sparse.hstack((q1_tfidf, q2_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First combine the questions and then calculate the tf-idf\n",
    "q1q2 = data.question1.fillna(\"\") \\\n",
    "     + \" \" \\\n",
    "     + data.question2.fillna(\"\")\n",
    "fs3_2 = tfv.fit_transform(q1q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the SVD matrices togetherr\n",
    "fs3_3 = np.hstack((question1_vectors, question2_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skew and kurtosis of the question vectors\n",
    "data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the skew and kurtosis features for future reference\n",
    "fs3_4 = ['skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORD2VEC BASED FEATURES**\n",
    "\n",
    "Simply put, Word2vec models are bi-layered neural networks that transform words to vectors so that words with similar meanings are close to eachother.\n",
    "Word2vec models take a corpus as input and output a vector for each word in that corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's Word2vec model\n",
    "model = gensim.models \\\n",
    "              .KeyedVectors \\\n",
    "              .load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\",\n",
    "                                    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google's Word2vec model expects words as input, so sentences must be\n",
    "# transformed to vectors indirectly\n",
    "def sent2vec(s, model):\n",
    "    words = word_tokenize(s.lower())\n",
    "    # Stopwords and numbers must be removed, as well as words that are not\n",
    "    # part of the model\n",
    "    M = [model[w] for w in words if w not in stop_words \\\n",
    "                             and w.isalpha() \\\n",
    "                             and w in model]\n",
    "    M = np.array(M)\n",
    "    if len(M) > 0:\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    else:\n",
    "        # When the sentence is empty after removing unvalid tokens, the vector\n",
    "        # is equal to the null-vector\n",
    "        return model.get_vector('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sent2vec vectors for every question\n",
    "w2v_q1 = np.array([sent2vec(q, model) \n",
    "                   for q in data.question1])\n",
    "w2v_q2 = np.array([sent2vec(q, model) \n",
    "                   for q in data.question2])\n",
    "# Stack the sent2vec vectors\n",
    "w2v = np.hstack((w2v_q1, w2v_q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_5 = ['w2v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate numerous sent2vec-based distance features\n",
    "data[\"cosine_distance\"] = [cosine(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"cityblock_distance\"] = [cityblock(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"jaccard_distance\"] = [jaccard(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"canberra_distance\"] = [canberra(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"euclidean_distance\"] = [euclidean(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"minkowski_distance\"] = [minkowski(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"braycurtis_distance\"] = [braycurtis(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sent2vec-distance-based feature set for future reference\n",
    "fs4_1 = ['cosine_distance', 'cityblock_distance', \n",
    "         'jaccard_distance', 'canberra_distance', \n",
    "         'euclidean_distance', 'minkowski_distance',\n",
    "         'braycurtis_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally calculate the word mover distance between two questions\n",
    "def wmd(s1, s2, model):\n",
    "    s1 = s1.lower().split()\n",
    "    s2 = s2.lower().split()\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the wmd for each pair of questions\n",
    "data[\"wmd\"] = \\\n",
    "    data.apply(lambda x: wmd(x.question1, x.question2, model),\n",
    "               axis=1)\n",
    "model.init_sims(replace=True)\n",
    "data[\"norm_wmd\"] = \\\n",
    "    data.apply(lambda x: wmd(x.question1, x.question2, model),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs4_2 = ['wmd', 'norm_wmd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "y = data.is_duplicate.values\n",
    "y = y.astype(\"float32\").reshape(-1,1)\n",
    "X = data[fs_1+fs_2+fs3_4+fs4_1+fs4_2]\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0).values\n",
    "X = scaler.fit_transform(X)\n",
    "X = np.hstack((X, fs3_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the X and y arrays for later testing, so you don't have to rerun the entire file\n",
    "# WARNING: the X.tsv file is almost 4GB in size\n",
    "np.savetxt(\"y.tsv\", y, delimiter=\"\\t\")\n",
    "np.savetxt(\"X.tsv\", X, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the X and y arrays for testing, this takes quite some time due to the size of X.tsv\n",
    "y = np.genfromtxt(\"y.tsv\", delimiter=\"\\t\")\n",
    "y = y.astype(\"float32\").reshape(-1,1)\n",
    "X = np.genfromtxt(\"X.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data\n",
    "np.random.seed(42)\n",
    "n_all, _ = y.shape\n",
    "idx = np.arange(n_all)\n",
    "np.random.shuffle(idx)\n",
    "n_split = n_all // 10\n",
    "idx_val = idx[:n_split]\n",
    "idx_train = idx[n_split:]\n",
    "x_train = X[idx_train]\n",
    "y_train = np.ravel(y[idx_train])\n",
    "x_val = X[idx_val]\n",
    "y_val = np.ravel(y[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the xgboost model\n",
    "params = dict()\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = [\"logloss\", \"error\"]\n",
    "params[\"eta\"] = 0.02\n",
    "params[\"max_depth\"] = 4\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_val, label=y_val)\n",
    "watchlist = [(d_train, \"train\"), (d_valid, \"valid\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params, d_train, 5000, watchlist,\n",
    "                early_stopping_rounds=50, verbose_eval=100)\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.5).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "bst.save_model(\"0001.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "bst_loaded = xgb.Booster({\"nthread\":4})\n",
    "bst_loaded.load_model(\"0001.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb accuracy : 0.782\n"
     ]
    }
   ],
   "source": [
    "# Test whether the loaded model works\n",
    "xgb_preds_test = (bst_loaded.predict(d_valid) >= 0.5).astype(int)\n",
    "xgb_accuracy_test = np.sum(xgb_preds_test == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
