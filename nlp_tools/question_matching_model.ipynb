{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on/inspired by code written by Packt: https://github.com/PacktPublishing/TensorFlow-Deep-Learning-Projects.\n",
    "The license that came with the original code can be found in the EXTERNAL_LICENSES folder.\n",
    "\n",
    "The data in \"quora_duplicate_questions.tsv\" is released for non-commercial use only by Quora.\n",
    "You can download this data from: http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv.\n",
    "More info can be found on: https://www.quora.com/about/tos.\n",
    "\n",
    "The pretrained Word2vec model from the Google News Corpus can be downloaded from: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz.\n",
    "**Warning:** the decompressed file is appr. 3.5GB in size and will be loaded into memory so 16GB of RAM and a 64-bit Python interpreter are advised.\n",
    "\n",
    "It is **important to note** that the data collected by Quora is a snapshot of a certain moment in history so questions about important events during that time may introduce biasses in the data. \n",
    "One way to obtain better models would be by collecting new domain-specific or general data, depending on the purpose of the recognizer.\n",
    "The data also contains some wrongly classified entries, which can be cleaned up to improve the model.\n",
    "\n",
    "It is also **important to note** that as long as the model is based of the quora data set, the model can only be used for **non-commercial** purposes. \n",
    "So creating a new, independent data set is of paramount importance, should you wish to use the subsequent models for commercial purposes.\n",
    "\n",
    "quora_duplicate_questions.tsv and GoogleNews-vectors-negative300.bin.gz must be downloaded to /nlp_tools.\n",
    "\n",
    "**This code expands the quora data set with useful features to train a machine learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk import word_tokenize\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, \\\n",
    "                                   euclidean, minkowski, braycurtis\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yves\n",
      "[nltk_data]     D'hondt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yves\n",
      "[nltk_data]     D'hondt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary nltk packages; only run this cell if you do not have\n",
    "# these packages installed\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ASSUMPTIONS**\n",
    "1. 2 questions that mean the same often share a lot of words, while 2 different questions rarely share a lot of words.\n",
    "2. 2 questions that mean the same often have a small edit distance, while 2 different questions rarely have a small edit distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse & Expand the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and remove unnecessary columns\n",
    "data = pd.read_csv(\"quora_duplicate_questions.tsv\", sep=\"\\t\") \\\n",
    "         .drop([\"id\", \"qid1\", \"qid2\"], axis=1)\n",
    "data.question1 = data.question1.apply(lambda x: str(x))\n",
    "data.question2 = data.question2.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LENGTH BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each sentence\n",
    "data[\"len_q1\"] = data.question1.apply(lambda x: len(x))\n",
    "data[\"len_q2\"] = data.question2.apply(lambda x: len(x))\n",
    "# Calculate the difference between the lengths of each pair of questions\n",
    "data[\"diff_len\"] = data.len_q1 - data.len_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the character length of each sentence (excluding spaces)\n",
    "data[\"len_char_q1\"] = data.question1.apply(lambda x: len(x.replace(\" \", \"\")))\n",
    "data[\"len_char_q2\"] = data.question2.apply(lambda x: len(x.replace(\" \", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the word count of each sentence\n",
    "data[\"len_word_q1\"] = data.question1.apply(lambda x: len(x.split()))\n",
    "data[\"len_word_q2\"] = data.question2.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of common words in each pair of questions\n",
    "data[\"common_words\"] = \\\n",
    "    data.apply(lambda x: len(set(x.question1.lower().split()).intersection(\n",
    "                             set(x.question2.lower().split()))),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length-based feature set for future reference\n",
    "fs_1 = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1',\n",
    "        'len_char_q2', 'len_word_q1', 'len_word_q2',\n",
    "        'common_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISTANCE BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Q and W ratio of each pair of questions\n",
    "data[\"fuzz_QRatio\"] = \\\n",
    "    data.apply(lambda x: fuzz.QRatio(x.question1,\n",
    "                                     x.question2),\n",
    "               axis=1)\n",
    "data[\"fuzz_WRatio\"] = \\\n",
    "    data.apply(lambda x: fuzz.WRatio(x.question1,\n",
    "                                     x.question2),\n",
    "               axis=1)\n",
    "# Calculate the partial ratio of each pair of questions\n",
    "data[\"fuzz_partial_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_ratio(x.question1,\n",
    "                                            x.question2),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the partial token set ratio of each pair of questions\n",
    "data[\"fuzz_partial_token_set_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_token_set_ratio(x.question1,\n",
    "                                                      x.question2),\n",
    "               axis=1)\n",
    "# Calculate the partial token sort ratio of each pair of questions\n",
    "data[\"fuzz_partial_token_sort_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.partial_token_sort_ratio(x.question1,\n",
    "                                                       x.question2),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the token set ratio of each pair of questions\n",
    "data[\"fuzz_token_set_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.token_set_ratio(x.question1,\n",
    "                                              x.question2),\n",
    "               axis=1)\n",
    "# Calculate the token sort ratio of each pair of questions\n",
    "data[\"fuzz_token_sort_ratio\"] = \\\n",
    "    data.apply(lambda x: fuzz.token_sort_ratio(x.question1,\n",
    "                                               x.question2),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distance-based feature set for future reference\n",
    "fs_2 = ['fuzz_QRatio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "        'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "        'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF & LSA BASED FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create term frequency-inverse document frequency vectorizers\n",
    "tfv = TfidfVectorizer(min_df=3,\n",
    "                      max_features=None,\n",
    "                      strip_accents='unicode',\n",
    "                      analyzer='word',\n",
    "                      token_pattern=r\"\\w{1,}\",\n",
    "                      ngram_range=(1, 2),\n",
    "                      use_idf=1,\n",
    "                      smooth_idf=1,\n",
    "                      sublinear_tf=1,\n",
    "                      stop_words=\"english\")\n",
    "tfv_q1 = deepcopy(tfv)\n",
    "tfv_q2 = deepcopy(tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tf-idf matrices for both questions\n",
    "q1_tfidf = tfv_q1.fit_transform(data.question1.fillna(\"\"))\n",
    "q2_tfidf = tfv_q2.fit_transform(data.question2.fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create truncated SVD decompostions = fast but aproximate, with 180 components\n",
    "svd = TruncatedSVD(n_components=180)\n",
    "svd_q1 = TruncatedSVD(n_components=180)\n",
    "svd_q2 = TruncatedSVD(n_components=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the SVD features based on the tf-idf matrices\n",
    "question1_vectors = svd_q1.fit_transform(q1_tfidf)\n",
    "question2_vectors = svd_q2.fit_transform(q2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3rd feature set is obtained by combining the tf-idf and SVD features\n",
    "# Stack the tf-idf matrices together\n",
    "fs3_1 = sparse.hstack((q1_tfidf, q2_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First combine the questions and then calculate the tf-idf\n",
    "q1q2 = data.question1.fillna(\"\") \\\n",
    "     + \" \" \\\n",
    "     + data.question2.fillna(\"\")\n",
    "fs3_2 = tfv.fit_transform(q1q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the SVD matrices togetherr\n",
    "fs3_3 = np.hstack((question1_vectors, question2_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skew and kurtosis of the question vectors\n",
    "data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the skew and kurtosis features for future reference\n",
    "fs3_4 = ['skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORD2VEC BASED FEATURES**\n",
    "\n",
    "Simply put, Word2vec models are bi-layered neural networks that transform words to vectors so that words with similar meanings are close to eachother.\n",
    "Word2vec models take a corpus as input and output a vector for each word in that corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's Word2vec model\n",
    "model = gensim.models \\\n",
    "              .KeyedVectors \\\n",
    "              .load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\",\n",
    "                                    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google's Word2vec model expects words as input, so sentences must be\n",
    "# transformed to vectors indirectly\n",
    "def sent2vec(s, model):\n",
    "    words = word_tokenize(s.lower())\n",
    "    # Stopwords and numbers must be removed, as well as words that are not\n",
    "    # part of the model\n",
    "    M = [model[w] for w in words if w not in stop_words \\\n",
    "                             and w.isalpha() \\\n",
    "                             and w in model]\n",
    "    M = np.array(M)\n",
    "    if len(M) > 0:\n",
    "        v = M.sum(axis=0)\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "    else:\n",
    "        # When the sentence is empty after removing unvalid tokens, the vector\n",
    "        # is equal to the null-vector\n",
    "        return model.get_vector('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sent2vec vectors for every question\n",
    "w2v_q1 = np.array([sent2vec(q, model) \n",
    "                   for q in data.question1])\n",
    "w2v_q2 = np.array([sent2vec(q, model) \n",
    "                   for q in data.question2])\n",
    "# Stack the sent2vec vectors\n",
    "w2v = np.hstack((w2v_q1, w2v_q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_5 = ['w2v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate numerous sent2vec-based distance features\n",
    "data[\"cosine_distance\"] = [cosine(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"cityblock_distance\"] = [cityblock(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"jaccard_distance\"] = [jaccard(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"canberra_distance\"] = [canberra(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"euclidean_distance\"] = [euclidean(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"minkowski_distance\"] = [minkowski(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]\n",
    "data[\"braycurtis_distance\"] = [braycurtis(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sent2vec-distance-based feature set for future reference\n",
    "fs4_1 = ['cosine_distance', 'cityblock_distance', \n",
    "         'jaccard_distance', 'canberra_distance', \n",
    "         'euclidean_distance', 'minkowski_distance',\n",
    "         'braycurtis_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally calculate the word mover distance between two questions\n",
    "def wmd(s1, s2, model):\n",
    "    s1 = s1.lower().split()\n",
    "    s2 = s2.lower().split()\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the wmd for each pair of questions\n",
    "data[\"wmd\"] = \\\n",
    "    data.apply(lambda x: wmd(x.question1, x.question2, model),\n",
    "               axis=1)\n",
    "model.init_sims(replace=True)\n",
    "data[\"norm_wmd\"] = \\\n",
    "    data.apply(lambda x: wmd(x.question1, x.question2, model),\n",
    "               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs4_2 = ['wmd', 'norm_wmd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "y = data.is_duplicate.values\n",
    "y = y.astype(\"float32\").reshape(-1,1)\n",
    "X = data[fs_1+fs_2+fs3_4+fs4_1+fs4_2]\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0).values\n",
    "X = scaler.fit_transform(X)\n",
    "X = np.hstack((X, fs3_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the X and y arrays for later testing, so you don't have to rerun the entire file\n",
    "# WARNING: the X.tsv file is almost 4GB in size\n",
    "np.savetxt(\"y.tsv\", y, delimiter=\"\\t\")\n",
    "np.savetxt(\"X.tsv\", X, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the X and y arrays for testing, this takes quite some time due to the size of X.tsv\n",
    "# y = np.genfromtxt(\"y.tsv\", delimiter=\"\\t\")\n",
    "# y = y.astype(\"float32\").reshape(-1,1)\n",
    "# X = np.genfromtxt(\"X.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data\n",
    "np.random.seed(42)\n",
    "n_all, _ = y.shape\n",
    "idx = np.arange(n_all)\n",
    "np.random.shuffle(idx)\n",
    "n_split = n_all // 10\n",
    "idx_val = idx[:n_split]\n",
    "idx_train = idx[n_split:]\n",
    "x_train = X[idx_train]\n",
    "y_train = np.ravel(y[idx_train])\n",
    "x_val = X[idx_val]\n",
    "y_val = np.ravel(y[idx_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the xgboost model\n",
    "params = dict()\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = [\"logloss\", \"error\"]\n",
    "params[\"eta\"] = 0.02\n",
    "params[\"max_depth\"] = 4\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_val, label=y_val)\n",
    "watchlist = [(d_train, \"train\"), (d_valid, \"valid\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.687516\ttrain-error:0.297336\tvalid-logloss:0.687544\tvalid-error:0.297583\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.502054\ttrain-error:0.261372\tvalid-logloss:0.503918\tvalid-error:0.263845\n",
      "[200]\ttrain-logloss:0.467881\ttrain-error:0.24483\tvalid-logloss:0.470465\tvalid-error:0.245937\n",
      "[300]\ttrain-logloss:0.451769\ttrain-error:0.233916\tvalid-logloss:0.454928\tvalid-error:0.237008\n",
      "[400]\ttrain-logloss:0.441147\ttrain-error:0.227128\tvalid-logloss:0.444955\tvalid-error:0.231195\n",
      "[500]\ttrain-logloss:0.433422\ttrain-error:0.222129\tvalid-logloss:0.43785\tvalid-error:0.226521\n",
      "[600]\ttrain-logloss:0.427421\ttrain-error:0.218177\tvalid-logloss:0.43239\tvalid-error:0.222514\n",
      "[700]\ttrain-logloss:0.422732\ttrain-error:0.214945\tvalid-logloss:0.428292\tvalid-error:0.219644\n",
      "[800]\ttrain-logloss:0.418426\ttrain-error:0.212125\tvalid-logloss:0.424548\tvalid-error:0.217418\n",
      "[900]\ttrain-logloss:0.414899\ttrain-error:0.209797\tvalid-logloss:0.421562\tvalid-error:0.215266\n",
      "[1000]\ttrain-logloss:0.411277\ttrain-error:0.207538\tvalid-logloss:0.418496\tvalid-error:0.213436\n",
      "[1100]\ttrain-logloss:0.407962\ttrain-error:0.205216\tvalid-logloss:0.415749\tvalid-error:0.211432\n",
      "[1200]\ttrain-logloss:0.405223\ttrain-error:0.203347\tvalid-logloss:0.413562\tvalid-error:0.209775\n",
      "[1300]\ttrain-logloss:0.402724\ttrain-error:0.201632\tvalid-logloss:0.411572\tvalid-error:0.208489\n",
      "[1400]\ttrain-logloss:0.400247\ttrain-error:0.200131\tvalid-logloss:0.409633\tvalid-error:0.207401\n",
      "[1500]\ttrain-logloss:0.397811\ttrain-error:0.198383\tvalid-logloss:0.407779\tvalid-error:0.205496\n",
      "[1600]\ttrain-logloss:0.395396\ttrain-error:0.196438\tvalid-logloss:0.40591\tvalid-error:0.204432\n",
      "[1700]\ttrain-logloss:0.393285\ttrain-error:0.194833\tvalid-logloss:0.404316\tvalid-error:0.203344\n",
      "[1800]\ttrain-logloss:0.391228\ttrain-error:0.193351\tvalid-logloss:0.402761\tvalid-error:0.202281\n",
      "[1900]\ttrain-logloss:0.389122\ttrain-error:0.192038\tvalid-logloss:0.401189\tvalid-error:0.201118\n",
      "[2000]\ttrain-logloss:0.387126\ttrain-error:0.1906\tvalid-logloss:0.399753\tvalid-error:0.200673\n",
      "[2100]\ttrain-logloss:0.38504\ttrain-error:0.18899\tvalid-logloss:0.398203\tvalid-error:0.199782\n",
      "[2200]\ttrain-logloss:0.38296\ttrain-error:0.187541\tvalid-logloss:0.396641\tvalid-error:0.198694\n",
      "[2300]\ttrain-logloss:0.38109\ttrain-error:0.1862\tvalid-logloss:0.395339\tvalid-error:0.197828\n",
      "[2400]\ttrain-logloss:0.379331\ttrain-error:0.1849\tvalid-logloss:0.394142\tvalid-error:0.197358\n",
      "[2500]\ttrain-logloss:0.377759\ttrain-error:0.183732\tvalid-logloss:0.393109\tvalid-error:0.19627\n",
      "Stopping. Best iteration:\n",
      "[2473]\ttrain-logloss:0.378202\ttrain-error:0.184035\tvalid-logloss:0.393412\tvalid-error:0.196023\n",
      "\n",
      "Xgb accuracy : 0.804\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(params, d_train, 5000, watchlist,\n",
    "                early_stopping_rounds=50, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "bst.save_model(\"0001.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "bst_loaded = xgb.Booster({\"nthread\":4})\n",
    "bst_loaded.load_model(\"0001.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb cut-off at 50%\n",
      "Xgb accuracy : 0.804\n",
      "Xgb false positives : 0.097 \n",
      "\n",
      "Xgb cut-off at 60%\n",
      "Xgb accuracy : 0.786\n",
      "Xgb false positives : 0.050 \n",
      "\n",
      "Xgb cut-off at 70%\n",
      "Xgb accuracy : 0.750\n",
      "Xgb false positives : 0.021 \n",
      "\n",
      "Xgb cut-off at 80%\n",
      "Xgb accuracy : 0.711\n",
      "Xgb false positives : 0.007 \n",
      "\n",
      "Xgb cut-off at 90%\n",
      "Xgb accuracy : 0.670\n",
      "Xgb false positives : 0.001 \n",
      "\n",
      "Xgb cut-off at 95%\n",
      "Xgb accuracy : 0.649\n",
      "Xgb false positives : 0.00025\n"
     ]
    }
   ],
   "source": [
    "# Print some statistics about the model\n",
    "print(\"Xgb cut-off at 50%\")\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.5).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy)\n",
    "xgb_false_positives = np.sum(np.logical_and(xgb_preds == 1,\n",
    "                                            y_val == 0)\n",
    "                            ) / len(y_val)\n",
    "print(\"Xgb false positives : %0.3f \\n\" % xgb_false_positives)\n",
    "\n",
    "print(\"Xgb cut-off at 60%\")\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.6).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy)\n",
    "xgb_false_positives = np.sum(np.logical_and(xgb_preds == 1,\n",
    "                                            y_val == 0)\n",
    "                            ) / len(y_val)\n",
    "print(\"Xgb false positives : %0.3f \\n\" % xgb_false_positives)\n",
    "\n",
    "print(\"Xgb cut-off at 70%\")\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.7).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy)\n",
    "xgb_false_positives = np.sum(np.logical_and(xgb_preds == 1,\n",
    "                                            y_val == 0)\n",
    "                            ) / len(y_val)\n",
    "print(\"Xgb false positives : %0.3f \\n\" % xgb_false_positives)\n",
    "\n",
    "print(\"Xgb cut-off at 80%\")\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.8).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy)\n",
    "xgb_false_positives = np.sum(np.logical_and(xgb_preds == 1,\n",
    "                                            y_val == 0)\n",
    "                            ) / len(y_val)\n",
    "print(\"Xgb false positives : %0.3f \\n\" % xgb_false_positives)\n",
    "\n",
    "print(\"Xgb cut-off at 90%\")\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.9).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy)\n",
    "xgb_false_positives = np.sum(np.logical_and(xgb_preds == 1,\n",
    "                                            y_val == 0)\n",
    "                            ) / len(y_val)\n",
    "print(\"Xgb false positives : %0.3f \\n\" % xgb_false_positives)\n",
    "\n",
    "print(\"Xgb cut-off at 95%\")\n",
    "xgb_preds = (bst.predict(d_valid) >= 0.95).astype(int)\n",
    "xgb_accuracy = np.sum(xgb_preds == y_val) / len(y_val)\n",
    "print(\"Xgb accuracy : %0.3f\" % xgb_accuracy)\n",
    "xgb_false_positives = np.sum(np.logical_and(xgb_preds == 1,\n",
    "                                            y_val == 0)\n",
    "                            ) / len(y_val)\n",
    "print(\"Xgb false positives : %0.5f\" % xgb_false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear trade-off between accuracy and false positives.\n",
    "We want a high accuracy because this means that if an answer is available, it will get found.\n",
    "The higher the accuracy, the less storage space we will need, since we won't have to save duplicate questions.\n",
    "On the other hand we want to avoid false positives at all costs.\n",
    "\n",
    "A false positive rate of 5% might seem small, at first, for instance.\n",
    "The binomial distribution quickly makes us realize otherwise!\n",
    "If we have 500 questions in our database, then the chance that a false positive will occur when searching over these 500 questions is nearly 100%.\n",
    "A false positive rate of 0,1% reduces this chance to 39,4%, which is still quite high.\n",
    "A false positive rate of 0,025% reduces this chance to 11,8%.\n",
    "Based on what we see here a cut-off of **>90%** seems optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Remarks\n",
    "I have tried multiple things to improve the model such as lemmatizing words, stemming words, adding part-of-speech tags, using different distance measures, deleting some features, etc. but they all ended up making the model worse in some way.\n",
    "The state of the model right now is the original configuration as proposed by Packt.\n",
    "The only way we could still improve the model, would be by using a more advanced tool (e.g. TensorFlow) instead of Xgboost.\n",
    "However, because of the speed and ease-of-use of the Xgboost model, I would suggest to stick with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
